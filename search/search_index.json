{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to the Documentation Hub for ELIXIR Cloud &amp; AAI, a Driver Project of the GA4GH.</p> <p>Whether you are a (potential) user, developer, systems administrator or contributor, this is your one-stop shop for documentation around the ELIXIR::GA4GH Cloud federated analytics platform and bespoke ELIXIR Cloud &amp; AAI solutions for your cloud needs.</p>  Users Developers Administrators Contributors <p>Conveniently run your genomics workflows and compute-intensive tasks in the cloud  </p> <p> Get started</p> <p>Integrate your tools and services with GA4GH-powered federated analytics environments</p> <p> Get started</p> <p>Make your data or compute cluster available to users inside or outside of your organization and outscale peak performance needs to the cloud</p> <p> Get started</p> <p>Contribute to the development of GA4GH-powered federated analytics infrastructure</p> <p> Get started</p>"},{"location":"about/code-of-conduct/","title":"ELIXIR Cloud &amp; AAI Code of Conduct","text":""},{"location":"about/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"about/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"about/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"about/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"about/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"about/code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"about/code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"about/code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"about/code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"about/code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"about/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"about/contact/","title":"Contact","text":"<p>Get in touch with us through one of the following methods:</p> Method \u2003 \u2003 \u2003 \u2003 \u2003 \u2003 \u2003 \u2003 Use it for image/svg+xml  \u00a0 Issue trackers Report bugs, suggest features or discuss existing issues; please try to create issues in the most relevant repository (or repositories); as an example, find here the issue tracker for the repository containing this documentation image/svg+xml  \u00a0 Slack Work closely together with us, e.g., if you would like to contribute or integrate your services or infrastructure with us image/svg+xml  \u00a0 Twitter Include us in your tweets; note that our Twitter account is silent for now, i.e., as an organization, we do not tweet ourselves image/svg+xml \u00a0 Q&amp;A forum Ask public Q&amp;A style questions that can be answered by or discussed with us or anyone else email \u00a0 Email First contact for collaborations or any other private communication; for continuous communication, please join our Slack board"},{"location":"about/license/","title":"Creative Commons Legal Code","text":""},{"location":"about/license/#cc0-10-universal","title":"CC0 1.0 Universal","text":"<p>CREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE LEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN ATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS INFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES REGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED HEREUNDER.</p>"},{"location":"about/license/#statement-of-purpose","title":"Statement of Purpose","text":"<p>The laws of most jurisdictions throughout the world automatically confer exclusive Copyright and Related Rights (defined below) upon the creator and subsequent owner(s) (each and all, an \"owner\") of an original work of authorship and/or a database (each, a \"Work\").</p> <p>Certain owners wish to permanently relinquish those rights to a Work for the purpose of contributing to a commons of creative, cultural and scientific works (\"Commons\") that the public can reliably and without fear of later claims of infringement build upon, modify, incorporate in other works, reuse and redistribute as freely as possible in any form whatsoever and for any purposes, including without limitation commercial purposes. These owners may contribute to the Commons to promote the ideal of a free culture and the further production of creative, cultural and scientific works, or to gain reputation or greater distribution for their Work in part through the use and efforts of others.</p> <p>For these and/or other purposes and motivations, and without any expectation of additional consideration or compensation, the person associating CC0 with a Work (the \"Affirmer\"), to the extent that he or she is an owner of Copyright and Related Rights in the Work, voluntarily elects to apply CC0 to the Work and publicly distribute the Work under its terms, with knowledge of his or her Copyright and Related Rights in the Work and the meaning and intended legal effect of CC0 on those rights.</p> 1. Copyright and Related Rights. <p>A Work made available under CC0 may be protected by copyright and related or neighboring rights (\"Copyright and Related Rights\"). Copyright and Related Rights include, but are not limited to, the following:  </p> <p>i. the right to reproduce, adapt, distribute, perform, display,  communicate, and translate a Work; ii. moral rights retained by the original author(s) and/or performer(s); iii. publicity and privacy rights pertaining to a person's image or  likeness depicted in a Work; iv. rights protecting against unfair competition in regards to a Work,  subject to the limitations in paragraph 4(a), below; v. rights protecting the extraction, dissemination, use and reuse of data  in a Work; vi. database rights (such as those arising under Directive 96/9/EC of the  European Parliament and of the Council of 11 March 1996 on the legal  protection of databases, and under any national implementation  thereof, including any amended or successor version of such  directive); and vii. other similar, equivalent or corresponding rights throughout the  world based on applicable law or treaty, and any national  implementations thereof.  </p> 2. Waiver. To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer's Copyright and Related Rights and associated claims and causes of action, whether now known or unknown (including existing as well as future claims and causes of action), in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the \"Waiver\"). Affirmer makes the Waiver for the benefit of each member of the public at large and to the detriment of Affirmer's heirs and successors, fully intending that such Waiver shall not be subject to revocation, rescission, cancellation, termination, or any other legal or equitable action to disrupt the quiet enjoyment of the Work by the public as contemplated by Affirmer's express Statement of Purpose.   3. Public License Fallback. Should any part of the Waiver for any reason be judged legally invalid or ineffective under applicable law, then the Waiver shall be preserved to the maximum extent permitted taking into account Affirmer's express Statement of Purpose. In addition, to the extent the Waiver is so judged Affirmer hereby grants to each affected person a royalty-free, non transferable, non sublicensable, non exclusive, irrevocable and unconditional license to exercise Affirmer's Copyright and Related Rights in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the \"License\"). The License shall be deemed effective as of the date CC0 was applied by Affirmer to the Work. Should any part of the License for any reason be judged legally invalid or ineffective under applicable law, such partial invalidity or ineffectiveness shall not invalidate the remainder of the License, and in such case Affirmer hereby affirms that he or she will not (i) exercise any of his or her remaining Copyright and Related Rights in the Work or (ii) assert any associated claims and causes of action with respect to the Work, in either case contrary to Affirmer's express Statement of Purpose.   4. Limitations and Disclaimers. a. No trademark or patent rights held by Affirmer are waived, abandoned,    surrendered, licensed or otherwise affected by this document. b. Affirmer offers the Work as-is and makes no representations or    warranties of any kind concerning the Work, express, implied,    statutory or otherwise, including without limitation warranties of    title, merchantability, fitness for a particular purpose, non    infringement, or the absence of latent or other defects, accuracy, or    the present or absence of errors, whether or not discoverable, all to    the greatest extent permissible under applicable law. c. Affirmer disclaims responsibility for clearing rights of other persons    that may apply to the Work or any use thereof, including without    limitation any person's Copyright and Related Rights in the Work.    Further, Affirmer disclaims responsibility for obtaining any necessary    consents, permissions or other rights required for any use of the    Work. d. Affirmer understands and acknowledges that Creative Commons is not a    party to this document and has no duty or obligation with respect to    this CC0 or use of the Work."},{"location":"about/release-notes/","title":"Release notes","text":"<p>Under construction</p> <p>More info coming soon...</p>"},{"location":"guides/guide-admin/","title":"Administrator guide","text":"<p>Welcome to the systems administrator guide to the ELIXIR Cloud. Whether you would like to onboard your data or compute center, set up your own GA4GH-based cloud or simply play around with our compute and storage solutions, this is the right place to get you off the ground.</p>"},{"location":"guides/guide-admin/#general-deployment-notes","title":"General deployment notes","text":"<p>Most of our services (see our GitHub organization for a comprehensive list) come with Helm charts for deployment on Cloud Native infrastructure and Docker Compose configurations for testing/development deployments. If you do not have experience with these technologies, please find some brief primers with references to additional documentation below.</p>"},{"location":"guides/guide-admin/#using-helm","title":"Using Helm","text":"<p>Helm is an IaC tool that is described as the \"package manager for Kubernetes\". It allows the management of the lifecycle of a Kubernetes application, i.e., its deployment, configuration, upgrade, retiring, etc. Applications ara packaged into \"Charts\".  Using Helm Charts allows us to version control an application and therefore follow its evolution over time, make identical copies (e.g., development, staging, production), make predictable upgrades, and share/publish the application. </p> <p>Some useful Helm commands to manage a Chart are:</p> <ul> <li><code>helm create</code>: Create a Helm Chart</li> <li><code>helm install</code>: Install an application</li> <li><code>helm upgrade</code>: Upgrade an application</li> <li><code>helm uninstall</code>: Uninstall an application</li> </ul>"},{"location":"guides/guide-admin/#using-docker-compose","title":"Using Docker Compose","text":"<p>Most of our services provide a Docker Compose configuration file for easy deployment of the software on a local machine. If the Docker Engine and Docker Compose are already installed on your system, it is as simple as cloning the service's Git repository, changing into the folder where the Docker Compose file resides (typically <code>docker-compose.yml</code> in a repository's root directory) and running the following:</p> <pre><code>docker-compose up -d\n</code></pre> <p>Non-standard name or location of config file</p> <p>The command will be different if the Docker Compose config file is not in the current working directory and/or is not called <code>docker-compose.yml</code>.</p> <p>This will bring the service up. The argument <code>-d</code> (or <code>--detach</code>) starts the app in daemonized mode, i.e., all launched containers that compose creates run in the background.</p> <p>In order to stop the deployment, simply run:</p> <pre><code>docker-compose down\n</code></pre>"},{"location":"guides/guide-admin/#onboarding-your-compute-center","title":"Onboarding your compute center","text":"<p>Follow the instructions below to onboard your compute node with the ELIXIR Cloud. Afterwards, your compute cluster will be accessible through the GA4GH Task Execution Service (TES) API and, optionally, available in the ELIXIR Cloud compute network.</p>"},{"location":"guides/guide-admin/#deploying-compute","title":"Deploying compute","text":"<p>Depending on whether you have a Native Cloud cluster or an HPC/HTC, you will need to follow the instructions for deploying TESK or Funnel below, respectively.</p>"},{"location":"guides/guide-admin/#deploying-tesk","title":"Deploying TESK","text":"<p>TESK uses the Kubernetes Batch API (Jobs) to schedule execution of TES tasks. This means that it should be possible to deploy TESK in any flavor of Kubernetes, but tests are currently only performed with Kubernetes, OpenShift, and Minikube. Follow these instructions if you wish to deploy a TES endpoint on your Native Cloud cluster, and please let us know if you deploy TESK in any new and interensting platform.</p> <p>TESK currently does not use any other storage (DB) than Kubernetes itself. Persistent Volume Claims are used as a temporary storage to handle input and output files of a task and pass them over between executors of a task. Note that PVCs are destroyed immediately after task completion! This means your cluster will need to provide a ReadWriteMany StorageClass. Commonly used storage classes are NFS and CephFS.</p> <p>Here is an overview of TESK's architecture:</p> <p>A Helm chart is provided for the convenient deployment of TESK. The chart is available in the TESK code repository.</p> <p>Follow these steps:</p> <ol> <li>Install Helm</li> <li> <p>Clone the TESK repository:</p> <pre><code>git clone https://github.com/elixir-cloud-aai/TESK.git\n</code></pre> </li> <li> <p>Find the Helm chart at <code>charts/tesk</code></p> </li> <li>Edit file <code>values.yaml</code> (see    notes below)</li> <li>Log into the cluster and install TESK with:</li> </ol> <pre><code>helm install -n TESK-NAMESPACE TESK-DEPLOYMENT-NAME . \\\n  -f secrets.yaml \\\n  -f values.yaml\n</code></pre> <ul> <li>Replace <code>TESK-NAMESPACE</code> with the name of the namespace where you want to    install TESK. If the namespace is not specified, the default namespace will    be used.</li> <li>The argument provided for <code>TESK-DEPLOYMENT-NAME</code> will be used by Helm to    refer to the deployment, for example when upgrading or deleting the    deployment. You can choose whichever name you like.</li> </ul> <p>You should now have a working TESK isntance!</p>"},{"location":"guides/guide-admin/#notes-for-editing-chart-values","title":"Notes for editing chart values","text":"<p>In the TESK deployment documentation documentation there is a description of every value. Briefly, the most important are:</p> <ol> <li><code>host_name</code>: Will be used to serve the API.</li> <li><code>storageClass</code>: Specify the storage class. If left empty, TESK will use the    default one configred in the Kubernetes cluster.</li> <li><code>auth.mode</code>: Enable (<code>auth</code>) or disable (<code>noauth</code>; default) authentication.    When enabled, an OIDC client must be in a file <code>./secrets.yaml</code>, with    the following format:</li> </ol> <pre><code>auth:\n    client_id: &lt;client_id&gt;\n    client_secret: &lt;client_secret&gt;\n</code></pre> <ol> <li><code>ftp</code>: Which FTP credentials mode to use. Two options are supported:    <code>.classic_ftp_secret</code> for basic authentication (username and password) or    <code>.netrc_secret</code> for using a <code>.netrc</code> file.</li> </ol> <p>For the classic approach, you must write in <code>values.yaml</code>:</p> <pre><code>ftp:\n    classic_ftp_secret: ftp-secret\n</code></pre> <p>And in a file <code>.secrets.yaml</code> write down the username and password as:</p> <pre><code>ftp:\n    username: &lt;username&gt;\n    password: &lt;password&gt;\n</code></pre> <p>For the <code>.netrc</code> approach, create a <code>.netrc</code> file in the <code>ftp</code> folder with    the connections details in the correct format.</p> <ol> <li><code>clusterType</code>: Type of Kubernetes flavor. Currently supported: <code>kubernetes</code>    (default) and <code>openshift</code>.</li> </ol> <p>Careful</p> <p>When creating a <code>.secrets.yaml</code> file, ensure that the file is never shared or committed to a code repository!</p>"},{"location":"guides/guide-admin/#notes-for-deployment-with-microk8s","title":"Notes for deployment with microk8s","text":"<p>This section outlines how to install TESK via microk8s as tested on an Ubuntu 22.04 machine.</p> <p>First, install microk8s through the Snap Store and add yourself to the <code>microk8s</code> group::</p> <pre><code>sudo snap install microk8s --classic\nsudo usermod -a -G microk8s $USER\n</code></pre> <p>Now let's create a directory for the microk8s configuration and enable Helm:</p> <pre><code>mkdir  ~/.kube\nsudo chown -R $USER ~/.kube\nmicrok8s enable helm3\n</code></pre> <p>Next, let's clone the TESK repository and move into it the Helm chart directory:</p> <pre><code>git clone https://github.com/elixir-cloud-aai/TESK.git\ncd TESK/charts/tesk\n</code></pre> <p>Follow the deployment instructions to create <code>secrets.yaml</code> and modify <code>values.yaml</code> as per your requirements.</p> <p>You MUST set <code>host_name</code>. To make the service available through the internet, see further below on how to configure the <code>service</code> section.</p> <p>Great - you are now ready to deploy TESK!</p> <p>First, let's create a namespace:</p> <pre><code>microk8s kubectl create namespace NAMESPACE\n</code></pre> <p>where <code>NAMESPACE</code> is an arbitrary name for your resource group.</p> <p>Now let's use Helm to install:</p> <pre><code>microk8s helm3 install -n NAMESPACE RELEASE_NAME . -f secrets.yaml -f values.yaml\n</code></pre> <p>where <code>RELEASE_NAME</code> is an arbitrary name for this particular TESK release.</p> <p>Congratulations - TESK should now be successfully deployed!</p> <p>To find out the IP address at which TESK is available, run the following command:</p> <pre><code>microk8s kubectl get svc -n NAMESPACE\n</code></pre> <p>The output could look something like this:</p> <pre><code>NAME       TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)    AGE\ntesk-api   ClusterIP   123.123.123.123   &lt;none&gt;        8080/TCP   8s\n</code></pre> <p>Use the <code>CLUSTER-IP</code> and the <code>PORT</code> with the following template to construct the URL at which the service is available (and make sure to replace the dummy URL when you want to try out the calls below):</p> <pre><code>http://CLUSTER-IP:PORT/ga4gh/tes/v1\n</code></pre> <p>So, in this example case, we get the following URL:</p> <pre><code>http://123.123.123.123:8080/ga4gh/tes/v1\n</code></pre> <p>You can now test the intallation with the following example call to get a list of tasks:</p> <pre><code>curl http://123.123.123.123:8080/ga4gh/tes/v1/tasks\n</code></pre> <p>If everything worked well, you should get an output like this:</p> <pre><code>{\n  \"tasks\": []\n}\n</code></pre> <p>Let's try to send a small task to TESK:</p> <pre><code>curl \\\n  -H \"Accept: application/json\"  \\\n  -H \"Content-Type: application/json\" \\\n  -X POST \\\n  --data '{\"executors\": [ { \"command\": [ \"echo\", \"TESK says: Hello World\" ], \"image\": \"alpine\" } ]}' \\\n  \"http://123.123.123.123:8080/ga4gh/tes/v1/tasks\"\n</code></pre> <p>That should give you a task ID:</p> <pre><code>{\n  \"id\" : \"task-123ab456\"\n}\n</code></pre> <p>You can run the task list command from before again. Now the response should not be an empty list anymore. Rather, you should see something like this:</p> <pre><code>{\n  \"tasks\" : [ {\n    \"id\" : \"task-123ab456\",\n    \"state\" : \"COMPLETE\"\n  } ]\n}\n</code></pre> <p>To get more details on your task, use the task ID from before in a call like this:</p> <pre><code>curl http://123.123.123.123:8080/ga4gh/tes/v1/tasks/TASK_ID?view=FULL\n</code></pre> <p>We can use <code>jq</code> to parse the results. Let's say we want to see the logs of the first (only, in this case) TES executor, we could do something like this:</p> <pre><code>$curl -s http://123.123.123.123:8080/ga4gh/tes/v1/tasks/task-123ab456?view=FULL  | jq '.logs[0].logs'\n</code></pre> <p>Which would give us an output like this:</p> <pre><code>[\n  {\n    \"start_time\": \"2023-11-01T14:54:20.000Z\",\n    \"end_time\": \"2023-11-01T14:54:25.000Z\",\n    \"stdout\": \"TESK says: Hello World\\n\",\n    \"exit_code\": 0\n  }\n]\n</code></pre> <p>Note that in the example, the API is only accessible internally. To make it accessible publicly, we need to properly configure the <code>service</code> section in <code>values.yaml</code>.</p> <p>In particular, we would like to set the type to <code>NodePort</code> and then set an open port on the host machine at which the API is exposed. For example, with</p> <pre><code>service:\n    type: NodePort\n    node_port: 31567\n</code></pre> <p>Kubernetes will route requests coming in to port <code>31567</code> on the host machine to port <code>8080</code> on the pod.</p> <p>Let's confirm this by upgrading the Helm chart and again inspecting the services in our namespace with:</p> <pre><code>microk8s helm3 upgrade -n NAMESPACE RELEASE_NAME . -f secrets.yaml -f values.yaml\nmicrok8s kubectl get svc -n NAMESPACE\n</code></pre> <p>We should get an output like this:</p> <pre><code>NAME       TYPE       CLUSTER-IP        EXTERNAL-IP   PORT(S)          AGE\ntesk-api   NodePort   123.123.123.111   &lt;none&gt;        8080:31567/TCP   5s\n</code></pre> <p>Indeed, the port section changed as expected. Now, note that the <code>CLUSTER-IP</code> also changed. However, this is not a problem as Kubernetes will manage the routing, so we don't really need to know the <code>CLUSTER-IP</code>. Instead, now we can use the hostname (or IP) of the host machine, together with the port we set to call our TES API from anywhere:</p> <pre><code>curl http://HOST_NAME_OR_IP:31567/ga4gh/tes/v1/tasks\n</code></pre> <p>Of course you need to make sure that the port you selected is opened for public access. This will depend on your router/firewall settings.</p> <p>If you would like to tear down the TESK service, simply run:</p> <pre><code>microk8s helm uninstall RELEASE_NAME -n NAMESPACE\n</code></pre>"},{"location":"guides/guide-admin/#deploying-funnel","title":"Deploying Funnel","text":"<p>Follow these instructions if you wish to deploy a TES endpoint in front of your HPC/HTC cluster (currently tested with Slurm and OpenPBS.</p> <ol> <li>Make sure the build dependencies <code>make</code> and Go 1.11+ are    installed, <code>GOPATH</code> is set and <code>GOPATH/bin</code> is added to <code>PATH</code>.</li> </ol> <p>For example, in Ubuntu this can be achieved via:</p> <pre><code>sudo apt update\nsudo apt install make golang-go\nexport GOPATH=/your/desired/path\nexport PATH=$GOPATH/bin:$PATH\ngo version\n</code></pre> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/ohsu-comp-bio/funnel.git\n</code></pre> <ol> <li>Build Funnel:</li> </ol> <pre><code>cd funnel\nmake\n</code></pre> <ol> <li>Test the installation by starting the Funnel server with:</li> </ol> <pre><code>funnel server run\n</code></pre> <p>If all works, Funnel should be ready for deployment on your HPC/HTC.</p>"},{"location":"guides/guide-admin/#slurm","title":"Slurm","text":"<p>For the use of Funnel with Slurm, make sure the following conditions are met:</p> <ol> <li>The <code>funnel</code> binary must be placed in a server with access to Slurm.</li> <li>A config file must be created and placed on the same server. This    file can be used as a starting point.</li> <li>If we would like to deploy Funnel as a Systemd service,    this file can be used as a template. Set the    correct paths to the <code>funnel</code> binary and config file.</li> </ol> <p>If successfull Funnel should be listening on port <code>8080</code>.</p>"},{"location":"guides/guide-admin/#openpbs","title":"OpenPBS","text":"<p>Under construction</p> <p>More info coming soon...</p>"},{"location":"guides/guide-admin/#deploying-storage","title":"Deploying storage","text":"<p>Follow the instructions below to connect your TES endpoint to one or more ELIXIR Cloud cloud storage solutions. The currently supported solutions are:</p> <ul> <li>MinIO (Amazon S3)</li> <li><code>vsftpd</code> (FTP)</li> </ul> <p>Other storage solutions</p> <p>Other S3 and FTP implementations may work but have not being tested.</p>"},{"location":"guides/guide-admin/#deploying-minio-amazon-s3","title":"Deploying MinIO (Amazon S3)","text":"<p>In order to deploy the MinIO server, follow the official documentation. It is very simple</p> <p>If you are deploying Minio to OpenShift, you may find this Minio-OpenShift template useful.</p>"},{"location":"guides/guide-admin/#deploying-vsftpd-ftp","title":"Deploying <code>vsftpd</code> (FTP)","text":"<p>There are a lot of guides available online to deploy <code>vsftpd</code>, for example this one. There are only two considerations:</p> <ol> <li>It is required to activate secure FTP support with <code>ssl_enable=YES</code>.</li> <li>For onboarding with the ELIXIR Cloud, currently the server should have one    account with a specific username and password created. Please contact    us for details.</li> </ol>"},{"location":"guides/guide-admin/#registering-your-tes-service","title":"Registering your TES service","text":"<p>We are currently working on implementing access control mechanisms and providing a user interface for the ELIXIR Cloud Registry. Once available, we will add registration instructions here. For now, please let us know about your new TES endpoint by email.</p>"},{"location":"guides/guide-admin/#custom-cloud-deployments","title":"Custom cloud deployments","text":"<p>Under construction</p> <p>More info coming soon...</p>"},{"location":"guides/guide-admin/services_to_ls_aai/","title":"Using LS-Login in JupyterHub","text":"<ul> <li>For deploying JupyterHub in de.NBI Cloud using Kubernetes see this tutorial.</li> <li>JupyterHub can be configured to use Life Science Login. Therefore, you need to have a service registered at LS-Login (HowTo).</li> <li>In the SPREG tool (link) you have to register the Redirect URIs accordingly to your domain at the SAML/OIDC setting page (<code>https://{yourdomain}/hub/oauth_callback</code> - be aware to change <code>{yourdomain}</code> accordingly). LS-LOGIN does exact matching of the URL, so adding <code>/hub/oauth_callback</code> is required and the domain alone is not sufficient.</li> <li>In the SAML/OIDC setting page, you will also find values for Client ID and Client Secret</li> <li>To use LS-Login in Jupyterhub, you have to modify the config.yaml file (see also here):</li> </ul> <pre><code>hub:\n  config:\n    Authenticator:\n      allow_all: true\n      admin_users:\n        - ADMIN # here you can specify an user as admin\n    GenericOAuthenticator:\n      client_id: # add client id from SPREG\n      client_secret: # add client secret from SPREG\n      login_service : LS LOGIN\n      oauth_callback_url: https://{yourdomain}/hub/oauth_callback # change to you domain accordingly\n      authorize_url: https://login.aai.lifescience-ri.eu/oidc/authorize\n      token_url: https://login.aai.lifescience-ri.eu/oidc/token\n      userdata_url: https://login.aai.lifescience-ri.eu/oidc/userinfo\n      username_claim: preferred_username # used in newer versions\n      username_key: preferred_username # this variable is deprecated in newer versions\n      scope: \n        - openid\n        - email\n        - profile\n    JupyterHub:\n      authenticator_class: generic-oauth\ncull:\n  enabled: false\n</code></pre>"},{"location":"guides/guide-admin/services_to_ls_aai/#add-ls_login-to-hedgedoc","title":"Add LS_Login to Hedgedoc","text":"<p>Before configuring Hedgedoc, you need to register your service with LS-Login. Follow the registration process at https://lifescience-ri.eu/ls-login/documentation/how-to-integrate/registration.html</p> <p>Hedgedoc is configured using environment variables. This guide assumes that a Hedgedoc is already deployed, in our case we used this chart:</p> <p>https://github.com/CSCfi/helm-charts/tree/main/charts/hedgedoc</p> <p>Once Hedgedoc is deployed, in order to add LS-AAI login one just needs to add these variables:</p> <ul> <li>name: CMD_OAUTH2_USER_PROFILE_URL</li> <li>value: https://login.aai.lifescience-ri.eu/oidc/userinfo</li> <li>name: CMD_OAUTH2_USER_PROFILE_USERNAME_ATTR</li> <li>value: preferred_username</li> <li>name: CMD_OAUTH2_USER_PROFILE_DISPLAY_NAME_ATTR</li> <li>value: name</li> <li>name: CMD_OAUTH2_USER_PROFILE_EMAIL_ATTR</li> <li>value: email</li> <li>name: CMD_OAUTH2_TOKEN_URL</li> <li>value: https://login.aai.lifescience-ri.eu/oidc/token</li> <li>name: CMD_OAUTH2_AUTHORIZATION_URL</li> <li>value: https://login.aai.lifescience-ri.eu/oidc/authorize</li> <li>name: CMD_OAUTH2_CLIENT_ID</li> <li>value: REPLACE BY CLIENT ID</li> <li>name: CMD_OAUTH2_CLIENT_SECRET</li> <li>value: REPLACE BY CLIENT SECRET</li> <li>name: CMD_OAUTH2_PROVIDERNAME</li> <li>value: ELIXIR Cloud &amp; AAI</li> <li>name: CMD_OAUTH2_SCOPE</li> <li>value: openid email profile</li> </ul> <p>The documentation from Hedgedoc about this is at:</p> <p>https://docs.hedgedoc.org/configuration/#oauth2-login</p>"},{"location":"guides/guide-admin/services_to_ls_aai/#using-ls-login-in-minio","title":"Using LS-Login in MinIO","text":"<p>LS-Login can be activated in MinIO either by using the MinIO console using the OIDC configuration or by setting environmental variables (MinIO OIDC Documentation).</p> <ul> <li>Config URL (MINIO_IDENTITY_OPENID_CONFIG_URL)</li> <li>https://login.aai.lifescience-ri.eu/oidc/.well-known/openid-configuration</li> <li>Client ID (MINIO_IDENTITY_OPENID_CLIENT_ID)</li> <li>ID of the LS-Login service</li> <li>Client secret (MINIO_IDENTITY_OPENID_CLIENT_SECRET)</li> <li>Secret of the LS-Login service</li> <li>Display Name (MINIO_IDENTITY_OPENID_DISPLAY_NAME)</li> <li>A human readable label for the login button (e.g. <code>LS-Login</code>)</li> <li>Scopes (MINIO_IDENTITY_OPENID_SCOPES)</li> <li>Scopes that will be requested from LS-Login (e.g. <code>openid,email,profile</code>)</li> <li>Role policy (MINIO_IDENTITY_OPENID_ROLE_POLICY)</li> <li>Name of a policy in MinIO that will be used to manage access of LS-Login users (e.g. <code>readonly</code>).</li> <li>Claim User Info (MINIO_IDENTITY_OPENID_CLAIM_USERINFO)</li> <li>Allow MinIO to request the userinfo endpoint for additional information (<code>on</code>).</li> </ul> <p>MinIO supports two different mechanisms for authorization of users with OIDC (MinIO OIDC authorization). It is recommended to use the RolePolicy flow. Here, all LS-Login users in MinIO will be assigned to one or more policies. These policies can control access to specific buckets by group membership; e.g. require that users belong to a specific LS-AAI group (see policy based access control).</p> <p>In the example below, access to a bucket (<code>sensitive/</code>) is restricted to a list of users which are identified by their <code>preferred_username</code> claims. </p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::sensitive\",\n                \"arn:aws:s3:::sensitive/*\"\n            ],\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"jwt:preferred_username\": [\n                        \"ELIXIR_USERNAME\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"guides/guide-contributor/","title":"Contributor guide","text":"<p>Great to see you here - we would be delighted to have you join our team! So if you are interested, just read on for information on how to contribute.</p> <p>If you don't know what ELIXIR Cloud &amp; AAI is all about, follow the link or check out these documentation pages. Otherwise, please read on to find out more about the general topics you can help us with, our core values, what we can offer you and what we expect from contributors.</p> <p>One-time contributors</p> <p>If you are a seasoned FOSS contributor and you just want to work on a single issue, you may want to jump directly to the contribution guidelines section.</p>"},{"location":"guides/guide-contributor/#key-topics","title":"Key topics","text":"<p>Here is a list of broad topics and challenges that you can address while working with us:</p> <ul> <li>Aid the development of consensus-driven community standards for the   ethical, secure and efficient sharing and analysis of sensitive data together   with international partners in big and small public and private organizations</li> <li>Help to develop high-quality, scalable, secure, cloud native web   applications that implement these standards, with the goal of enabling   thousands of professional users in academia, the public healthcare sector and   industry to analyze sensitive data in a federated cloud environment</li> <li>Support the production-grade, containerized deployment of these services   on modern infrastructure in private, public, hybrid and multicloud settings</li> <li>Help design and implement the security features governing the services   individually and in combination, including the flow of authentication and   authorization according to community standards</li> </ul>"},{"location":"guides/guide-contributor/#our-values","title":"Our values","text":"<p>Here are some of the core values of the project and the team:</p> <p>Openness</p> <p>All of our code is open source, generally from day one, and available publicly under a license approved by the Open Source Initiative</p> <p>FAIRness</p> <p>We are committed to making data, software and infrastructure findable, accessible, interoperable and reusable</p> <p>Privacy &amp; Security</p> <p>We believe that sensitive personal data should be under the control of the individual and should be shared, processed and analyzed only upon and to the degree covered by informed and specific consent, using the highest security standards</p>"},{"location":"guides/guide-contributor/#our-offer","title":"Our offer","text":"<p>We offer you</p> <ul> <li>to work with us in a thriving and welcoming international community of   experts with diverse technical and cultural backgrounds</li> <li>the opportunity to tackle modern, highly relevant technical challenges on the   way to personalized medicine and genome-scale analytics in the cloud</li> <li>support, feedback and appreciation for your work, as well as involvment in   and influence on policy decisions proportional to the level of your   engagement</li> <li>networking opportunities within the ELIXIR and GA4GH   communities, with many opinion leaders in academia and the tech industry</li> </ul> <p>Regular contributors</p> <p>Check out the additional exiciting opportunities for regular contributors!</p>"},{"location":"guides/guide-contributor/#our-expectations","title":"Our expectations","text":"<p>If all of that sounds tempting and you are still interested to contribute, we would only expect you to please</p> <ul> <li>respect our Code of Conduct</li> <li>make an effort to adhere to good coding practices and our [contribution guidelines] (we will of course help you with that!)</li> <li>communicate clearly and openly with us with regard to any issues that you take responsibility for, especially if you realize that you cannot or do not want to continue with something (which is of course totally fine!)</li> </ul>"},{"location":"guides/guide-contributor/#opportunities","title":"Opportunities","text":"<p>For regular contributors, we offer a number of exciting opportunities, such as the chance of participating in GSOC, attending a hackathon or securing a paid internship with us. Please read on for more details on each of these.</p> <p>Letters of Recommendation</p> <p>If you have worked with us for at least 12 weeks full-time equivalent (GSoC, internship or otherwise), we will be happy to provide you with a letter of recommendation.</p>"},{"location":"guides/guide-contributor/#google-summer-of-code","title":"Google Summer of Code","text":"<p>Since 2019, we have been participating in the Google Summer of Code (GSoC) image/svg+xml , as part of the GA4GH mentor organization. You can reach out to us about GSoC at any time, however, to be able to join in any given year, you should do so at the latest when the host organization and projects are announced early in the calendar year (check the official timeline for the exact date).</p> <p>So far we had the immense pleasure to mentor the following excellent people during their GSoC stints (follow the links to find out more about the projects they were working on):</p> Year Contributor Project 2019 Vani Singh Task Distribution Logic for GA4GH Cloud APIs 2020 Sartah Gupta Dynamically adding data to DRS 2021 Akash Saini Brokering Continuous Delivery through the ELIXIR Cloud service registry 2021 Anurag Gupta Implement reusable GA4GH UI clients 2021 Vipul Chhabra Implement GA4GH TES in Galaxy/Pulsar 2022 Ayush Kumar Federating workflow execution using GA4GH Cloud APIs 2022 Lakshya Garg Compliance testing framework for the Task Execution Service API 2022 Suyash Gupta TES Callback endpoint and mechanism in TESK 2023 Assel Abzalova Web Components for discovering and using scientific software in the Cloud 2023 Javed Habib Dashboard Web Components (React, Vue etc) for Workflow and Task Runs 2023 Lakshya Garg Common compliance testing framework for every OpenAPI specification 2024 Aarav Mehta Extensible GA4GH Client Library/SDK and Command Line Interface implemented in Rust 2024 Athitheya Gobinathan Crypt4GH Support for GA4GH Task Execution Service API Implementations 2024 Javed Habib TESK - A GA4GH-TES based Kubernetes batch execution service 2024 Karanjot Singh Python library for packaging and reusing Cloud Computing workload traces 2024 Pratiksha Sankhe Dashboard Web Components (React, Vue etc) for file upload and handling"},{"location":"guides/guide-contributor/#hackathons","title":"Hackathons","text":"<p>We regularly lead ELIXIR Cloud &amp; AAI-related projects at various (bio)hackathons. These are always fun events and often a highlight of the year! Check our news page for any upcoming events. If you are interested in attending a hackathon, please reach out to us. In some cases, we may be able to procure funding for contributors.</p> <p>Here is a list of hackathons we have attended in the past:</p> <ul> <li>ELIXIR BioHackathon 2018, Paris, France</li> <li>NBDC/DBCLS BioHackathon 2019 in Fukuoka, Japan</li> <li>ELIXIR BioHackathon 2019, Paris, France</li> <li>COVID19 BioHackathon 2020, virtual</li> <li>ELIXIR BioHackathon 2020, virtual</li> <li>ELIXIR BioHackathon 2021, Barcelona, Spain</li> <li>ELIXIR BioHackathon 2022, Paris, France</li> <li>BioHackathon Germany 2022, Wittenberg, Germany</li> <li>BioHackathon MENA 2023, Thuwal, Saudi Arabia</li> <li>NBDC/DBCLS BioHackathon 2023 on Shodoshima Island, Japan</li> </ul>"},{"location":"guides/guide-contributor/#internships","title":"Internships","text":"<p>We offer regular high-quality contributors the opportunity of doing paid, open-ended, highly flexible \"freelancing\" internships with us, with an hourly rate roughly based on GSoC stipends. We will reach out to you if we feel that you qualify.</p>"},{"location":"guides/guide-contributor/PULL_REQUEST_TEMPLATE/","title":"PULL REQUEST TEMPLATE","text":""},{"location":"guides/guide-contributor/PULL_REQUEST_TEMPLATE/#description","title":"Description","text":"<ul> <li>Fixes #(issue number)</li> </ul>"},{"location":"guides/guide-contributor/PULL_REQUEST_TEMPLATE/#checklist","title":"Checklist","text":"<ul> <li> My code follows the contributing guidelines of this     project, including, in particular, with regard to any style guidelines</li> <li> The title of my PR complies with the Conventional Commits     specification; in particular, it clearly indicates     that a change is a breaking change</li> <li> I acknowledge that all my commits will be squashed into a single commit,     using the PR title as the commit message</li> <li> I have performed a self-review of my own code</li> <li> I have commented my code in hard-to-understand areas</li> <li> I have updated the user-facing documentation to describe any new or     changed behavior</li> <li> I have added type annotations for all function/class/method interfaces     or updated existing ones (only for Python, TypeScript, etc.)</li> <li> I have provided appropriate documentation (Google-style     Python docstrings or JSDoc block tags ) for all     packages/modules/functions/classes/methods or updated existing ones</li> <li> My changes generate no new warnings</li> <li> I have added tests that prove my fix is effective or that my feature     works</li> <li> New and existing unit tests pass locally with my changes</li> <li> I have not reduced the existing code coverage</li> </ul>"},{"location":"guides/guide-contributor/PULL_REQUEST_TEMPLATE/#comments","title":"Comments","text":""},{"location":"guides/guide-contributor/general-guidelines/","title":"General guidelines","text":"<p>For your contributions, please follow the guidelines laid out below to the best of your ability.</p> Beginners <p>If you don't have a lot of experience with this sort of workflow, the guidelines in this section may seem overwhelming. But please don't worry! We will guide you through the process and you will soon get the hang of it. And please don't worry about making mistakes either - everybody does them. Often! Our project layout makes it very very hard for anyone to cause irreversible harm, so relax, try things out, take your time and enjoy the work! :)</p>"},{"location":"guides/guide-contributor/general-guidelines/#communication","title":"Communication","text":"<p>Please use the comment functions available on GitHub to discuss issues and pull requests. For all other communications please refer to the communication channels listed in the contact section. In particular, use the chat to discuss project ideas, get help on a problem, or any other informal discussion that does not need to be preserved as part of the repository you are working on.</p>"},{"location":"guides/guide-contributor/general-guidelines/#submitting-issues","title":"Submitting issues","text":"<p>Please use each project's GitHub issue tracker to:</p> <ul> <li>propose features</li> <li>report bugs</li> <li>find issues to work on</li> <li>discuss existing issues</li> </ul> <p>As an example, you can find this project's issue tracker here.</p> <p>When submitting new issues to propose features or report bugs, please choose the most appropriate template, if available, then follow the instructions in the template.</p> <p>You don't need to add labels or milestones for an issue, the project maintainers will do that for you. However, it is important that all issues are written concisely, yet with enough detail and relevant context (links, screenshots, etc.) to allow others to start working on them. For bug reports, it is essential that they include reproducible examples.</p> <p>Here and here are some resources to help you get started on writing good issues.</p>"},{"location":"guides/guide-contributor/general-guidelines/#branching-model","title":"Branching model","text":"<p>All of our projects are version-controlled via Git and codebases are hosted on GitHub. Please refer to appropriate documentation and tutorials if you are not familiar with them.</p> <p>To keep track our project histories clean, we follow a slightly modified version of the GitHub Flow Branching model. What this means is that code changes are always merged into protected branches via pull requests that will undergo one or more rounds of review and testing. The schema below visualizes this process.</p> <p></p> <ol> <li>Create a \"feature branch\" from the development branch; use the    <code>feature/my-feature</code> naming pattern to name your feature branch, e.g.,    <code>feature/update-docs</code>; make sure the development branch    is up to date before creating the feature branch!</li> <li>Commit code changes to address the issue you are working on</li> <li>Push the feature branch to the remote and create a pull    request against the development branch in GitHub</li> <li>Address any comments added during code review by pushing additional commits    (there may be multiple rounds of reviews)</li> <li>Once all issues are resolved, code owners will merge the feature branch into    the development branch using the \"squash merging\"    method</li> </ol> <p>Development branch</p> <p>Typically, <code>dev</code> is the name of the development branch in our projects. In a few cases (e.g., this repository), there is no development branch and changes are merged immediately into the main branch, typically called <code>main</code>.</p> <p>Default branch</p> <p>For projects in pre-release state, i.e., those with version numbers below <code>v1.0.0</code>, the development branch is typically the default branch (meaning that the correct target branch for your PRs is set automatically).</p> <p>With the first major release, the default branch is set to the main branch instead. In that case, make sure to manually select the development branch as the target branch for your PRs.</p> Substantial changes <p>If your proposed changes will be substantial, try to split up the work into multiple feature branches. This makes reviewing easier, helps to keep the project history clean and may better guard against code regression. A rule of thumb is that you should be able to adequately summarize your changes with 50 characters.</p> <p>If your changes are becoming more substantial than you anticipated, you can request that the branch be \"rebase merged\" instead of \"squash merged\". However, we will only do so if you have exactly one clean commit for each semantic work package and each commit message follows the conventional commit specifications. You can use <code>git rebase --interactive</code> to clean up your feature branch.</p> Feature branches unstable <p>All feature branches are to be considered unstable, i.e., their history may change at any time, even after being pushed to the remote. Therefore, do not work on feature branches without clearly communicating with the people who have created them.</p>"},{"location":"guides/guide-contributor/general-guidelines/#commit-messages","title":"Commit messages","text":"<p>Generally, no specific formatting of individual commit messages is required when working on feature branches. However, your pull request titles MUST follow the Conventional Commits specification. The same is true for individual commit messages if you are requesting that your feature branch be \"rebase merged\" (see info box \"Substantial changes\" above).</p> <p>Conventional Commits help to increase consistency, facilitate maintenance and enable automated versioning and change log generation. Their general structure is as follows:</p> <pre><code>&lt;type&gt;(optional scope): &lt;description&gt;\n\n[optional body]\n\n[optional footer]\n</code></pre> <p>Please follow these rules for your commit messages / PR titles:</p> <ul> <li>Keep your entire header/title line (including type and, if available, scope)   at 50 characters or less</li> <li>Only use the types listed in the table below; choose the type according to the   predominant reason for the change</li> <li>Only use types <code>feat</code>, <code>fix</code>, <code>perf</code>, <code>refactor</code> and <code>style</code> for changes in   package/production code; use the dedicated types for all build-, CI-,   documentation- or test-related changes</li> <li>Indicating a scope is optional; it is only necessary if scopes are generally   used in the repository you are working on</li> <li>Start the <code>&lt;description&gt;</code> with a verb in imperative form (e.g., <code>add</code>, <code>fix</code>)</li> <li>If you include a body and/or footer, make sure it conforms to the   Conventional Commits specification</li> </ul> <p>Depending on the changes, we would kindly request you to use one of the following type prefixes:</p> Type Description build For changes related to the build system (e.g., scripts, configurations and tools) and package dependencies chore For changes related to mundane repository maintenance tasks that are not covered by any of the other types (e.g., adding a `.gitignore file) ci For changes related to the continuous integration and deployment system (e.g., workflows, scripts, configurations and tools) docs For changes related to the project documentation, regardless of the audience (end users, developers) feat For changes related to new abilities or functionality fix For changes related to bug fixes perf For changes related to performance improvements refactor For changes related to modifying the codebase, which neither adds a feature nor fixes a bug (e.g., removing redundant code, simplifying code, renaming variables) revert For changes that revert one or more previous commits style For changes related to styling the codebase (e.g., indentation, parentheses/brackets, white space, trailing commas) test For changes related to tests <p>Linting commit messages</p> <p>In order to ensure that the format of your commit messages adheres to the Conventional Commits specification and the defined type vocabulary, you can use a dedicated linter. More information can also be found in this blog post.</p>"},{"location":"guides/guide-contributor/general-guidelines/#filing-pull-requests","title":"Filing pull requests","text":"<p>Open pull requests through the GitHub interface, VS Code or your favorite Git client. Make sure to follow the branching model. Most importantly:</p> Use Conventional Commit messages for your PR titles! <p>See details in the commit message section.</p>"},{"location":"guides/guide-contributor/general-guidelines/#code-reviews","title":"Code reviews","text":"<p>All code changes are reviewed by at least one other person. This is to ensure that the code is of high quality, that it is well-documented and that it adheres to the project's coding standards. The reviewer will check that the code is correct, that it is efficient and that it is maintainable. They will also check that the code is well-documented and that it is tested.</p> <p>Please make sure to actively request reviews for your pull requests to avoid delays in the review and merging process. Please use the GitHub functionality for that (upper right hand corner of pull request view). If you are unsure who to ask for a review, please reach out to the project leads.</p>"},{"location":"guides/guide-contributor/general-guidelines/#pull-request-template","title":"Pull request template","text":"<p>The pull request template will be successively added to all repositories. Until that is the case, you can already make use of it by self-reviewing your pull requests according to the checklist and descriptions.</p>"},{"location":"guides/guide-contributor/managing-projects/","title":"Managing your own project","text":"<p>If you are managing a project by yourself or with others, please follow these additional guidelines below.</p>"},{"location":"guides/guide-contributor/managing-projects/#community-standards","title":"Community standards","text":"<p>Please try to adhere to best community standards. To help with that, visit GitHub's \"Community Standards\" section (accessible in each repository via the \"Insights\" tab) and confirm that the following are available in the repository root directory:</p> <ul> <li>README in file <code>README.md</code></li> <li>Code of Conduct in file <code>CODE_OF_CONDUCT.md</code> (can link to the Code of   Conduct on this page)</li> <li>Contributing guidelines in file <code>CONTRIBUTING.md</code> (can link to the   contributor guide on this page)</li> <li>License in file <code>LICENSE</code></li> <li>Pull request template in file <code>PULL_REQUEST_TEMPLATE.md</code></li> </ul> <p>Also make sure that a project description (right hand panel on the repository's main page) and one or more issue templates (in <code>.github/ISSUE_TEMPLATE</code>) are available.</p>"},{"location":"guides/guide-contributor/managing-projects/#licensing","title":"Licensing","text":"<p>All projects must be licensed under an Open Source Initiative (OSI)-approved license. The license must be included in the repository root directory in a file named <code>LICENSE</code>. Unless otherwise discussed, please use the Apache License 2.0.</p>"},{"location":"guides/guide-contributor/managing-projects/#versioning","title":"Versioning","text":"<p>All our projects are versioned according to the Semantic Versioning specification. This means that each version number consists of three parts: <code>MAJOR.MINOR.PATCH</code>. The version number is increased as follows:</p> <ul> <li><code>MAJOR</code> version is increased when you make incompatible/breaking API changes</li> <li><code>MINOR</code> version is increased when you add functionality in a backwards   compatible manner</li> <li><code>PATCH</code> version is increased when you make backwards compatible bug fixes</li> </ul> <p>Note that projects in pre-release state, i.e., should be assigned a version number below <code>1.0.0</code> (start with <code>0.1.0</code>). In Semantic Versioning, this means that API changes can occur at any moment, which is suitable for a project that has not reached sufficient maturity and API stability yet.</p>"},{"location":"guides/guide-contributor/managing-projects/#continuous-integration","title":"Continuous Integration","text":"<p>We are fully embracing the concept of continuous integration (CI) and related practices. This means that all code changes are automatically tested and validated before they are merged into the main codebase. This is to ensure that the codebase remains stable and that new features and bug fixes do not break existing functionality.</p> <p>Therefore, when starting a new project, as soon as possible, please add one or more GitHub Actions workflows to your project that do the following for pushes to and pull requests against the repository's default branch (see existing projects for examples):</p> <ul> <li>Run linting and formatting checks</li> <li>Run type checks (if applicable)</li> <li>Run unit and integration tests</li> <li>Check test coverage and upload results to Codecov</li> <li>Build and publish documentation (if not set up to be triggered automatically   by the publishing system, e.g., Read the Docs)</li> </ul> <p>Continuous Delivery/Deployment</p> <p>If the project you are working on is reasonably mature, also consider setting up one or more continuous delivery/deployment workflows.</p>"},{"location":"guides/guide-contributor/managing-projects/#documentation","title":"Documentation","text":"<p>Related to continuous integration, we also value continuous documentation. This means that documentation is updated and improved as the codebase evolves. This is to ensure that the documentation remains accurate and up-to-date and that it reflects the current state of the codebase. It also means that we want to start writing documentation as early as possible.</p> <p>Currently, we are using Markdown to write documentation. For new projects, we expect that each project has a <code>README.md</code> file that covers the following sections (fill in with \"Coming soon\" if not yet available):</p> <ul> <li>Synopsis: A brief description of the project</li> <li>Basic usage: A brief overview of how to use the project</li> <li>Installation: Instructions on how to install/deploy the project</li> <li>Versioning: Information on how the project is versioned</li> <li>Contributing: Guidelines on how to contribute to the project, with links     to the contributing guidelines and our     code of conduct</li> <li>Contact: Information on how to contact the project leads</li> </ul>"},{"location":"guides/guide-contributor/managing-projects/#hosted-documentation","title":"Hosted documentation","text":"<p>As projects grow, a simple <code>README.md</code> will not be sufficient anymore.</p> <p>We therefore kindly ask you to prepare a dedicated, hosted documentation page early on. This can be done using services like [Read the Docs][read-the-docs] or GitHub Pages with MKDocs (like this page).</p> <p>Carefully consider the audiences for your project and tailor the documentation accordingly. In all cases, we expect that API documentation is made available (can be auto-generated from code, e.g., via Sphinx).</p>"},{"location":"guides/guide-contributor/workflow/","title":"Workflow","text":"<p>To start working with us, please follow these simple steps:</p> <ol> <li>Join  image/svg+xml  GitHub.</li> <li>Check out our repositories and open    issues.</li> <li>Join our  image/svg+xml  Slack board (please let us    know if the link expired).</li> <li>Join the <code>#oss-community</code> and leave    a short message about yourself. Please include (1) your relevant skills and    experience level, (2) your GitHub username, (3) your email address (e.g.,    for calendar invitations), and (4) the repositories or issues you are most    interested in. If you can't decide, no problem, just indicate that you are    open to work on anything and we will suggest some issues for you.</li> <li>Once we have added you to our GitHub organization,    you can assign yourself to an issue.</li> <li>Please carefully read the guidelines below, as well    as any relevant language-specific guidelines.    If you are managing a project, please also consult the project management    guidelines.</li> <li>Start coding! </li> </ol> <p>Privacy note</p> <p>If you do not want to share your GitHub username and/or your email address in the public channel, please send a direct message to a project lead instead.</p> <p>Always work on issues</p> <p>If you want to propose code changes and there is no corresponding issue available, please make sure to open an issue first and get some feedback from project leads. This is to avoid the frustration of putting in work for nothing in case the project leads had other plans.</p> One-time contributors <p>If you like, you can skip steps 3. to 5. and raise pull requests from forks. Note, however, that some CI workflows may not (yet) be fully supported for pull requests raised from forks, which may delay the review process and the merging of your changes.</p>"},{"location":"guides/guide-contributor/language-specific/","title":"Language-specific guidelines","text":"<p>To make it easier for everyone to maintain, read and contribute to the code, as well as to ensure that the code base is robust and of high quality, we would kindly ask you to stick to the following language-specific guidelines for code style and testing. If you happen to start a project in a language that is not listed here, please contact us to discuss and develop a section for that language.</p> <p>Currently available languages:</p> <ul> <li>Python</li> </ul>"},{"location":"guides/guide-contributor/language-specific/python/","title":"Python guidelines","text":"<p>For all Python code, please stick to the guidelines described in this section.</p>"},{"location":"guides/guide-contributor/language-specific/python/#python-version","title":"Python version","text":"<p>For any new projects, please use one of the two most recent Python minor versions, exclusing pre-releases. For existing projects, use the Python version used throughout the project (mentioned in <code>pyproject.toml</code>).</p>"},{"location":"guides/guide-contributor/language-specific/python/#packaging-build-system-dependencies","title":"Packaging, build system &amp; dependencies","text":"<p>Please use <code>pyproject.toml</code> to configure packaging, building, and dependency management. Please do not use <code>setup.py</code> and do no specify your dependencies in <code>requirements.txt</code>, except for legacy projects where they are still used.</p> <p>Preferably, segregate dependencies for different tasks. For example, use <code>[tool.poetry.test.dependencies]</code> for testing dependencies, and <code>[tool.poetry.dependencies]</code> for runtime dependencies.</p> <p>Please sort entries (sections, dependencies) in <code>pyproject.toml</code> alphabetically to ease maintenance. If using Poetry (see below), you can use the <code>poetry-sort</code> plugin to help with this. Otherwise any other TOML sorter should work as well.</p> <p>Poetry</p> <p>We strongly recommend using the Poetry package manager instead of <code>pip</code>. In that case, use <code>poetry.lock</code> to lock the dependencies (make sure to commit the file to version control). To add a new dependency, use the following command:</p> <pre><code>poetry add &lt;package&gt; --group=&lt;group&gt;\n</code></pre> <p>To build the project, use:</p> <pre><code>poetry build\n</code></pre>"},{"location":"guides/guide-contributor/language-specific/python/#console-scripts","title":"Console scripts","text":"<p>If your project has one or more console script entry points, use <code>pyproject.toml</code> file to define them, e.g.:</p> <pre><code>[tool.poetry.scripts]\nmy-script = \"my_package.my_module.my_submodule:my_function\"\n</code></pre>"},{"location":"guides/guide-contributor/language-specific/python/#code-formatting-and-linting","title":"Code formatting and linting","text":""},{"location":"guides/guide-contributor/language-specific/python/#ruff","title":"Ruff","text":"<p>In an effort to reduce dependencies, we recommend using <code>ruff</code> for new projects and configuring it in <code>pyproject.toml</code>. Configure strictness based on project requirements, but at least use the following:</p> <pre><code>[tool.ruff.lint]\nselect = [\n  \"B\", # flake8-bugbear\n  \"D\", # pydocstyle\n  \"E\", # pycodestyle\n  \"F\", # Pyflakes\n  \"I\", # isort\n  \"PL\", # pylint\n  \"SIM\", # flake8-simplify\n  \"UP\", # pyupgrade\n]\n</code></pre> <p>Ruff usage</p> <p>You can fix lints by running <code>ruff check --fix /src/my_module.py</code> and <code>ruff format /src/my_module.py</code> to format the code.</p>"},{"location":"guides/guide-contributor/language-specific/python/#docstrings","title":"Docstrings","text":"<p>Please use Google-style docstrings for all packages, modules, classes, methods and functions. With <code>pydocstyle</code>, you can enforce this style with the following entry in the <code>pyproject.toml</code> file:</p> <pre><code>[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n</code></pre> <p>For methods and functions, please include at least the following sections, where applicable:</p> <ul> <li><code>Args</code></li> <li><code>Returns</code> (or <code>Yields</code>, for generator functions)</li> <li><code>Raises</code></li> </ul> <p>For classes, please include the <code>Attributes</code> section.</p> <p>Furthermore, in all cases, consider including <code>Examples</code> and <code>Note</code> sections.</p>"},{"location":"guides/guide-contributor/language-specific/python/#type-hints","title":"Type hints","text":"<p>Add type hints to all function and method signatures, as well as any global variables. Adding type hints to local variables is recommended if types aren't obvious from assignments.</p> <p>Adding type hints to (unit) tests is not necessary.</p> <p>Automating type hinting</p> <p>You can try using MonkeyType to help with adding type hints to your code.</p>"},{"location":"guides/guide-contributor/language-specific/python/#static-type-checkers","title":"Static type checkers","text":"<p>Please use a type checker to check for type consistency across your project. We recommend using <code>mypy</code>, but alternatives are acceptable.</p>"},{"location":"guides/guide-contributor/language-specific/python/#testing","title":"Testing","text":"<p>Please provide extensive tests (both unit and integration) for your code and determine the test coverage with the <code>coverage</code> package. Strive for a test coverage of 100% for unit and 70% for integration tests, because:</p> <p>Untested code is broken code.</p> <p>Please use <code>pytest</code> as a runner for your tests (it also comes with many useful features and extensions).</p> <p>Be aware that proposed code changes must never reduce the previous test coverage.</p>"},{"location":"guides/guide-dev/","title":"Developer guide","text":"<p>Under construction</p> <p>More info coming soon...</p>"},{"location":"guides/guide-user/","title":"User guide","text":""},{"location":"guides/guide-user/#introduction","title":"Introduction","text":"<p>Welcome to the user documentation for the ELIXIR Cloud &amp; AAI ecosystem. With this powerful set of services, you'll be able to easily access cloud resources and send analysis pipelines to your data with just a few simple commands. Imagine being able to run complex genomic analyses on massive datasets without worrying about infrastructure limitations or having to manage complex server environments. The GA4GH Cloud APIs give you access to powerful tools and resources that allow you to focus on your research goals, not IT.</p> <p>The GA4GH (Global Alliance for Genomics and Health) cloud APIs are a set of standard APIs that provide a common interface for accessing genomic data and tools across different cloud providers. These APIs are essential for enabling genomic data sharing and collaboration, and they have been adopted by major cloud providers such as Google Cloud Platform, Microsoft Azure, and Amazon Web Services. In this documentation, we'll cover four main GA4GH APIs that you'll be using: the Workflow Execution Service (WES), the Task Execution Service (TES), the Data Repository Service (DRS), and the Tool Registry Service (TRS). The WES API allows you to define and execute workflows, while the TES API allows you to execute individual tasks within those workflows. The DRS API provides a way to access and download genomic data, and the TRS API enables the discovery of genomic analysis tools.</p> <p>Whether you are a bioinformatician or a data scientist, this documentation will provide you with all the information you need to start using ELIXIR's GA4GH cloud services ecosystem and harness the power of cloud computing for your genomic data analysis needs. Let's get started!</p>"},{"location":"guides/guide-user/#elixir-cloud-aai-deployments","title":"ELIXIR Cloud &amp; AAI deployments","text":"<p>The ELIXIR Cloud &amp; AAI group manages different services and appliocations as part of the ELIXIR cloud framework. Currently, these services are temporarily listed in a dedicated services list applications.  In the mid-term, all services instances will be registered in the ELIXIR Cloud Registry, an implementation of the GA4GH Service Registry API.</p>"},{"location":"guides/guide-user/#task-execution-service-tes","title":"Task Execution Service (TES)","text":"<p>The GA4GH TES specification is a standard interface that enables interoperability between workflow management systems and execution engines. The TES specification provides a uniform way to submit and monitor tasks to any execution engine that implements the specification, allowing users to easily switch between workflow management systems or execution engines without rewriting their workflows. Typical use cases are</p> <ul> <li>Scenario 1: A researcher wants to run a workflow locally. The workflow   contains some resource-intensive steps, such as requirements for GPUs or many   cores. Using TES as a backend, the researcher can execute the workflow   locally and also send the resource-intensive tasks to cloud servers for   execution.</li> <li>Scenario 2: A researcher wants to run a workflow that involves processing   data that is stored in cloud locations. Using TES would allow individual   tasks to be sent to the compute locations associated with each storage   location. This may be relevant if the data provider does not allow files to   be downloaded to a central location or if it is not technically feasible.</li> </ul> <p>The TES specification defines a HTTP API for submitting and monitoring tasks that includes endpoints for creating, querying, updating, and canceling tasks. Tasks are defined as JSON objects that include input and output files, a command to execute, and any environment variables or resources required by the task. The TES specification also includes mechanisms for handling task dependencies and retrying failed tasks. Popular TES implementations are Funnel and TESK.</p> <p>Several popular workflow management systems, including cwl-tes, Snakemake and Nextflow, have implemented the TES specification, allowing users to easily run their workflows on any execution engine that supports TES.</p>"},{"location":"guides/guide-user/#snakemake","title":"Snakemake","text":"<p>Snakemake supports TES v1.0 since version 5.28.0, as described in the Snakemake documentation. Snakemake executes individual tasks as separate workflows that execute only one or a few rules. When using TES, it is recommended to use additional remote storage to store input and output files. By default, Snakemake TES tasks are executed using the official Snakemake container image in the same version as the original Snakemake call. To use specific tools, conda environments should be appended to the rules. A demo workflow is available here.</p>"},{"location":"guides/guide-user/#cwl-tes","title":"CWL-tes","text":"<p>A demo workflow is available here.</p>"},{"location":"guides/guide-user/#nextflow","title":"Nextflow","text":"<p>Under construction</p> <p>More info coming soon...</p>"},{"location":"guides/guide-user/#workflow-execution-service-wes","title":"Workflow Execution Service (WES)","text":"<p>The GA4GH WES is a standard specification protocol for executing and monitoring bioinformatics workflows. It allows researchers to easily execute and manage complex analysis pipelines across multiple computing platforms and institutions. The WES specification provides a unified API for describing workflow inputs and outputs, monitoring job status and progress, and managing data transfers. With this specification, users can build scalable, reproducible, and interoperable genomics workflows, enabling collaboration across institutions and improving data sharing. Two use cases for the GA4GH WES specification are:</p> <ul> <li> <p>Scenario 1: A researcher wants to analyze a large dataset of genomic data   using a specific analysis pipeline. With the WES specification, the   researcher can easily define the inputs and parameters for the pipeline,   select a computing platform that meets their requirements, and submit the job   for execution. They can then monitor the progress of the job and receive   notifications when the job is complete. This allows the researcher to focus   on analyzing the results rather than managing the underlying infrastructure.</p> </li> <li> <p>Scenario 2: A clinical laboratory needs to process patient samples for   genetic testing. The laboratory can use the WES specification to define the   analysis pipeline and integrate it with its LIMS. This allows the laboratory   to automate the processing of samples, reducing errors and turnaround time.</p> </li> </ul>"},{"location":"guides/guide-user/#data-repository-service-drs","title":"Data Repository Service (DRS)","text":"<p>Under construction</p> <p>More info coming soon...</p>"},{"location":"guides/guide-user/#tool-registry-service-trs","title":"Tool Registry Service (TRS)","text":"<p>Under construction</p> <p>More info coming soon...</p>"},{"location":"guides/guide-user/user_stories/","title":"User Stories","text":"<p>With the elixir cloud activities, we support the development of cloud services to provide access to data and compute for researchers. Thereby we have various user stories in mind, which are listed below to guide researchers as well as service/data provider.</p>"},{"location":"guides/guide-user/user_stories/#access-to-services","title":"Access to services","text":"<p>As a researcher, I can access a cloud service, which provides access to research data, so that I can perform data analysis on the research data. LS Login allows researchers to authenticate using their home institute credentials. Service providers must register the service with LS Login and, if necessary, authorize researchers or groups of researchers to use the service. Technically, the user navigates through the service's GUI to a login page. The service redirects the user to the LS Login page, where the user logs in with the personal credentials of their respective identity provider. LS Login checks whether the user has access rights to the service through group memberships or specific user roles/affiliations. If successful, the user gets redirected back to the service and the service receives the user information from LS Login.</p> <p>As a service provider, I can register my service, so that external researchers can authenticate and access the service and work with research data. For using LS Login, a service provider needs to register their service as a Relying Party (RP) at LS Login. There are instructions on how to connect a service available (How to connect a service to the LifeScience AAI). Once registered, service providers need to configure their service for using LS Login as a feature for authentication. See the following tutorials for configuring specific service: * Using LS-Login in JupyterHub * Using LS-Login in Hedgedoc</p> <p>As a service provider, I define the modalities for access to my service, so that access to my service is controlled to a set of specific users. These modalities can include membership to a user group or a specific user role/affiliation. LS-AAI manages all user group assignments and access modalities for services based on group memberships. There is tutorial provided by LS-Login for managing access to the relying parties). In short: * In SPREG the service provider needs to activate \"Restrict access to the service based on membership in groups\" * User of the service need to be member of the group. See Instructions for group managers. * A group can be assigned to a service (facility) in Perun.</p> <p>As a service provider, I can withdraw access rights, so that users won't be able to access the service anymore. Access to the service can be restricted again via the respective group memberships. See Instructions for group managers.</p>"},{"location":"guides/guide-user/user_stories/#access-to-data","title":"Access to data","text":"<p>As a researcher, I can identify a relevant sensitive data set, so that I can consider it for analysis in my own research. A prerequisite for the reuse of data is usually that the data fulfils the FAIR (Findable, Accessible, Interoperable, Reusable) criteria, including the findability of the data in metadatabases. Best practices and guidelines on FAIR criteria and data management are available in the ELIXIR RDMkit.</p> <p>As a researcher, I can request access to a sensitive data set, so that I can integrate it into my own research activities. The researcher creates a data access request for a specific data set via an access management system. An example of an access management system is REMS from CSC Finland. </p> <p>As a data controller, I can give a researcher access to a specific dataset, so that the researcher can work with the data in accordance with my data usage policies. The data controllers are responsible for providing access to sensitive data. Regarding sensitive human data, patients have usually specified the purposes for which personal data may be accessed and the controllers must check whether these purposes are fulfilled in the event of a request. For this purpose, GA4GH defines the Data Use Ontology to standardize both consents and requests. Approved access requests can be stored uniformly via the GA4GH Passport specification. Life Science Login acts as a passport broker and can provide user authorizations for the cloud services (GA4GH passport support in LifeScience AAI).</p>"},{"location":"guides/guide-user/user_stories/#analysis","title":"Analysis","text":"<p>As a researcher, I can run a specific data analysis workflow on an external dataset in the cloud, so that I can integrate the results into my research activities. The GA4GH standardizes the execution of workflows and workflow tasks in the cloud via the specification of the Workflow Execution Service (WES) and the Task Execution Service (TES) respectively. The ELIXIR on cloud project supports the execution of workflows and tasks by developing tools and providing guidance for researchers (user-guide). A user can trigger the execution of a workflow via a WES implementation. Once the workflow has been successfully completed, the resulting data can be used by the researcher for further activities.</p>"}]}